{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "bert-base-multilingual-cased: https://huggingface.co/bert-base-multilingual-cased\n",
    "\n",
    "Notes: does not recognize negatives, max token length is 512, which is less than other models. Does not work for paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "from transformers import BertTokenizer, BertModel\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-multilingual-cased')\n",
    "model = BertModel.from_pretrained(\"bert-base-multilingual-cased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 = [\"Ginger helps cure cancer.\"]\n",
    "encoded_input1 = tokenizer(text1, return_tensors='pt')\n",
    "output1 = model(**encoded_input1)\n",
    "\n",
    "text2 = [\"Ingver aitab ravida vähki.\"]\n",
    "encoded_input2 = tokenizer(text2, return_tensors='pt')\n",
    "output2 = model(**encoded_input2)\n",
    "\n",
    "text3 = [\"Ingver ei aita ravida vähki.\"]\n",
    "encoded_input3 = tokenizer(text3, return_tensors='pt')\n",
    "output3 = model(**encoded_input3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LI\\AppData\\Local\\Temp\\ipykernel_16044\\2320419056.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  npout1 = torch.tensor(output1[1][0]).numpy()\n",
      "C:\\Users\\LI\\AppData\\Local\\Temp\\ipykernel_16044\\2320419056.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  npout2 = torch.tensor(output2[1][0]).numpy()\n",
      "C:\\Users\\LI\\AppData\\Local\\Temp\\ipykernel_16044\\2320419056.py:5: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  npout3 = torch.tensor(output3[1][0]).numpy()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "npout1 = torch.tensor(output1[1][0]).numpy()\n",
    "npout2 = torch.tensor(output2[1][0]).numpy()\n",
    "npout3 = torch.tensor(output3[1][0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.0000001  0.92852294]\n",
      " [0.92852294 1.0000001 ]]\n"
     ]
    }
   ],
   "source": [
    "# Ginger helps cure cancer. + Ingver aitab ravida vähki.\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "\n",
    "vectors_matrix1 = np.array([npout1, npout2])\n",
    "\n",
    "similarity_matrix1 = cosine_similarity(vectors_matrix1)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.0000001  0.93704706]\n",
      " [0.93704706 0.9999998 ]]\n"
     ]
    }
   ],
   "source": [
    "# Ginger helps cure cancer. + Ingver ei aita ravida vähki.\n",
    "\n",
    "vectors_matrix2 = np.array([npout1, npout3])\n",
    "\n",
    "similarity_matrix2 = cosine_similarity(vectors_matrix2)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additional testing with paragraphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this model has a maximum token length of 512, which is less than the other models\n",
    "\n",
    "# first paragraph from snippet\n",
    "# second, snippet translated to eng\n",
    "# third, eng snippet text shuffled\n",
    "# fourth, unrelated parangraph\n",
    "\n",
    "first = \"Olgugi et Tallinna Halduskohus on tunnistanud vaktsiinisunni kaitseväes õigusvastaseks, toonitas Martin Herem kõigile kaitseväe teenistujatele saadetud kirjas, et koroonavaktsiinide manustamine on jätkuvalt teenistuse jätkamise eelduseks ehk kõik teenistujad, kes ei lase endale uut vaktsiinidoosi süstida, peavad kaitseväest lahkuma. Kaitseväe juhataja Martin Herem saatis 6. oktoobril kaitseväe siselisti kaudu laiali kirja, milles informeeris kõiki alluvaid oma otsusest nõuda kõigilt kaitseväe teenistujatelt jätkuvalt Covid-19 vaktsiinide manustamist teenistuse jätkamise eeldusena. Kuigi Tallinna Halduskohus tunnistas oma 30. augusti otsusega vaktsineerimiskohustusele mitteallunud kaitseväe teenistujate ametist vabastamise õigusvastaseks, annab Herem koha oma kirja pealkirjas teada, et kaitsevägi nõuab endiselt vaktsineerimistõendi esitamist. Kohe kirja alguses kinnitab Herem, et kaitseväe juhtkonnal ei ole kavatsust vaktsineerimiskohustusest loobuda.\"\n",
    "second = \"Even though the Tallinn Administrative Court has recognized forced vaccination in the defense forces as illegal, Martin Herem stressed in a letter sent to all servicemen of the defense forces that the administration of corona vaccines is still a prerequisite for continued service, i.e. all servicemen who do not allow themselves to be injected with a new vaccine dose must leave the defense forces. The head of the defense forces Martin Herem sent On October 6, a letter was distributed through the internal list of the defense forces, in which they informed all subordinates of their decision to continue to demand the administration of Covid-19 vaccines from all servicemen of the defense forces as a prerequisite for continuing their service. Although the Tallinn Administrative Court recognized with its August 30 decision dismissal of defense force personnel who are not subject to the vaccination obligation to be unlawful, gives In the title of his letter, Herem states that the defense force still requires proof of vaccination. Right at the beginning of the letter, Herem confirms that the leadership of the defense force has no intention of abandoning the vaccination obligation.\"\n",
    "third = \"Despite the Tallinn Administrative Court rejecting the notion of forced vaccination being unlawful in the defense forces, Martin Herem emphasized in a letter addressed to all servicemen that the corona vaccine administration remains a vital requirement for ongoing service. In other words, servicemen who refuse the new vaccine dose injection must exit the defense forces. On October 6, the head of the defense forces, Martin Herem, distributed a letter through the internal list of the defense forces, notifying subordinates of their decision to persist in demanding Covid-19 vaccine administration from all servicemen as a condition for continuing their service. Although the Tallinn Administrative Court dismissed the dismissal of defense force personnel not obligated to vaccination on August 30, Herem's letter insists that the defense force still requires proof of vaccination. At the commencement of the letter, Herem affirms the leadership's unwavering commitment to the vaccination obligation. The Defense Forces has required a valid proof of vaccination against COVID-19 from its servicemen. This requirement is still valid today.\"\n",
    "fourth = \"Amidst the vibrant tapestry of a bustling cityscape, the echoes of life intertwine with the rhythm of urban existence. In the heart of this metropolis, diverse communities coalesce, each with its narrative, a mosaic of stories that collectively shape the city's identity. From the towering skyscrapers that pierce the heavens to the intimate alleys that harbor hidden gems, every corner pulsates with the energy of human endeavor. Street vendors peddle their wares, filling the air with the aroma of exotic spices and the melody of bargaining. Parks and squares serve as communal canvases, where artists paint the aspirations and reflections of a multifaceted society. As day surrenders to night, the city transforms into a spectacle of lights, a nocturnal ballet choreographed by the ebb and flow of its denizens. Amid the ceaseless motion, moments of stillness emerge — a solitary figure reading a book on a park bench, a street musician serenading passersby, or a couple sharing a quiet conversation in a dimly lit cafe.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\LI\\AppData\\Local\\Temp\\ipykernel_16044\\2685113588.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  npout1 = torch.tensor(output1[1][0]).numpy()\n",
      "C:\\Users\\LI\\AppData\\Local\\Temp\\ipykernel_16044\\2685113588.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  npout2 = torch.tensor(output2[1][0]).numpy()\n",
      "C:\\Users\\LI\\AppData\\Local\\Temp\\ipykernel_16044\\2685113588.py:17: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  npout3 = torch.tensor(output3[1][0]).numpy()\n",
      "C:\\Users\\LI\\AppData\\Local\\Temp\\ipykernel_16044\\2685113588.py:18: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  npout4 = torch.tensor(output4[1][0]).numpy()\n"
     ]
    }
   ],
   "source": [
    "# this model has a maximum token length of 512, so I cut the texts in half compared to other models\n",
    "\n",
    "encoded_input1 = tokenizer(first, return_tensors='pt')\n",
    "output1 = model(**encoded_input1)\n",
    "\n",
    "encoded_input2 = tokenizer(second, return_tensors='pt')\n",
    "output2 = model(**encoded_input2)\n",
    "\n",
    "encoded_input3 = tokenizer(third, return_tensors='pt')\n",
    "output3 = model(**encoded_input3)\n",
    "\n",
    "encoded_input4 = tokenizer(fourth, return_tensors='pt')\n",
    "output4 = model(**encoded_input4)\n",
    "\n",
    "npout1 = torch.tensor(output1[1][0]).numpy()\n",
    "npout2 = torch.tensor(output2[1][0]).numpy()\n",
    "npout3 = torch.tensor(output3[1][0]).numpy()\n",
    "npout4 = torch.tensor(output4[1][0]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.         0.27657366]\n",
      " [0.27657366 0.99999976]]\n"
     ]
    }
   ],
   "source": [
    "vectors_matrix1 = np.array([npout1, npout2])\n",
    "\n",
    "similarity_matrix1 = cosine_similarity(vectors_matrix1)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.         0.19013044]\n",
      " [0.19013044 0.99999976]]\n"
     ]
    }
   ],
   "source": [
    "vectors_matrix1 = np.array([npout1, npout3])\n",
    "\n",
    "similarity_matrix1 = cosine_similarity(vectors_matrix1)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity Matrix:\n",
      "[[1.        0.9770721]\n",
      " [0.9770721 0.9999999]]\n"
     ]
    }
   ],
   "source": [
    "vectors_matrix1 = np.array([npout1, npout4])\n",
    "\n",
    "similarity_matrix1 = cosine_similarity(vectors_matrix1)\n",
    "\n",
    "print(\"Cosine Similarity Matrix:\")\n",
    "print(similarity_matrix1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
